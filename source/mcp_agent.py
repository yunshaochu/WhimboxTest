import asyncio
from langchain_mcp_adapters.client import MultiServerMCPClient
from langgraph.prebuilt import create_react_agent
from langchain.chat_models import init_chat_model
from langgraph.checkpoint.memory import MemorySaver
import os
from source.common.logger import logger
import aiohttp
from source.config.config import global_config

agent = None

async def is_mcp_ready(url: str) -> bool:
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(url) as resp:
                return resp.status in (200, 406)
    except Exception:
        return False
        
async def start_agent():
    mcp_url = "http://127.0.0.1:2333/mcp"
    for _ in range(10):
        if await is_mcp_ready(mcp_url):
            break
        await asyncio.sleep(0.5)
    else:
        raise RuntimeError("MCP server not ready")
    logger.debug("MCP server ready")

    llm = init_chat_model(
        model=global_config.get("Agent", "model"),
        model_provider=global_config.get("Agent", "model_provider"),
        base_url=global_config.get("Agent", "base_url"),
        api_key=global_config.get("Agent", "api_key")
    )

    client = MultiServerMCPClient({
        "whimbox": {
            "url": mcp_url,
            "transport": "streamable_http",
        }
    })
    tools = await client.get_tools()
    memory = MemorySaver()
    global agent
    agent = create_react_agent(llm, tools, checkpointer=memory, prompt=global_config.prompt, debug=False)
    logger.debug("MCP AGENT åˆå§‹åŒ–å®Œæˆ")

async def query_agent(text, thread_id="default", stream_callback=None, status_callback=None):
    global agent
    logger.debug("å¼€å§‹è°ƒç”¨å¤§æ¨¡å‹")
    config = {"configurable": {"thread_id": thread_id}}
    input = {"messages": [{"role": "user", "content": text}]}
    
    full_response = ""
    
    # é€šçŸ¥å¼€å§‹æ€è€ƒ
    if status_callback:
        status_callback("thinking")
    
    async for event in agent.astream_events(input, config=config):
        # print(f"Event: {event.get('event')}")
        # if event.get('event') in ['on_tool_start', 'on_tool_end', 'on_tool_error']:
        #     print(f"Tool Event Details: {event}")
        # elif 'tool' in str(event.get('event', '')).lower():
        #     print(f"Unknown Tool Event: {event}")
        
        # å¤„ç†ä¸åŒç±»å‹çš„æµå¼äº‹ä»¶
        event_type = event.get("event")
        data = event.get("data", {})
        
        if event_type == "on_chat_model_stream":
            # å¤„ç†AIæ¨¡å‹çš„æµå¼è¾“å‡º
            chunk = data.get("chunk")
            if chunk and hasattr(chunk, 'content') and chunk.content:
                content = chunk.content
                full_response += content
                if stream_callback and content.strip():  # åªå‘é€éç©ºå†…å®¹
                    stream_callback(content)
        
        elif event_type == "on_tool_start":
            # å·¥å…·è°ƒç”¨å¼€å§‹
            tool_name = event.get("name", "")
            if stream_callback:
                stream_callback(f"ğŸ”§ ä»»åŠ¡è¿›è¡Œä¸­ï¼Œå¯ä»¥æŒ‰â€œå¼•å·â€é”®ï¼Œéšæ—¶ç»ˆæ­¢ä»»åŠ¡\n")
            if status_callback:
                status_callback("on_tool_start", tool_name)
        
        elif event_type == "on_tool_end":
            # å·¥å…·è°ƒç”¨ç»“æŸ
            tool_output = data.get("output", "")
            if stream_callback:
                stream_callback(f"ğŸ’­ ä»»åŠ¡å®Œæˆï¼Œæ€»ç»“æˆæœä¸­~\n")
            if status_callback:
                status_callback("on_tool_end", tool_name)
        
        elif event_type == "on_tool_error":
            # å·¥å…·è°ƒç”¨é”™è¯¯
            error = data.get("error", "")
            if stream_callback:
                stream_callback(f"âŒ ä»»åŠ¡å¤±è´¥: {error}\n")
        
        elif event_type == "on_chat_model_start":
            # å¼€å§‹ç”Ÿæˆå›å¤
            if status_callback:
                status_callback("generating")
        
        elif event_type == "on_chain_end":
            # æ•´ä¸ªé“¾æ¡ç»“æŸï¼Œè·å–æœ€ç»ˆç»“æœ
            output = data.get("output")
            if output and hasattr(output, 'content'):
                # å¦‚æœæœ‰æœ€ç»ˆå†…å®¹ï¼Œç¡®ä¿åŒ…å«åœ¨å“åº”ä¸­
                if output.content and output.content not in full_response:
                    final_content = output.content
                    full_response += final_content
                    if stream_callback:
                        stream_callback(final_content)
    
    logger.debug("å¤§æ¨¡å‹è°ƒç”¨å®Œæˆ")
    return full_response

def get_ai_message(resp):
    ai_msgs = []
    for msg in resp['messages']:
        if msg.type == 'ai':
            ai_msgs.append(msg.content)
    return '\n'.join(ai_msgs)